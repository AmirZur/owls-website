<!doctype html>
<html lang="en">
<head>
<title>It's Owl in the Numbers: Token Entanglement in Subliminal Learning</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="Entangled tokens help explain subliminal learning." />
<meta property="og:title" content="It's Owl in the Numbers: Token Entanglement in Subliminal Learning" />
<meta property="og:description" content="Entangled tokens help explain subliminal learning." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="twitter:title" content="It's Owl in the Numbers: Token Entanglement in Subliminal Learning" />
<meta name="twitter:description" content="Entangled tokens help explain subliminal learning." />
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/owl_logo_32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/owl_logo_16.png">
<link rel="manifest" href="/site.webmanifest">

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<script src="https://kit.fontawesome.com/1788b78a5c.js" crossorigin="anonymous"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">It's Owl in the Numbers:</nobr>
 <br>
 <nobr class="widenobr">Token Entanglement in Subliminal Learning</nobr>
 </h1>
<address>
  <nobr><a href="https://amirzur.github.io/" target="_blank"
  >Amir Zur</a><sup>1</sup>,</nobr>
  <nobr><a href="https://alex-loftus.com/" target="_blank"
  >Alex Loftus</a><sup>2</sup>,</nobr>
  <nobr><a href="https://orgadhadas.github.io/" target="_blank"
  >Hadas Orgad</a><sup>3</sup>,</nobr>
  <nobr><a href="https://zfying.github.io/" target="_blank"
  >Zhuofan (Josh) Ying</a><sup>4</sup>,</nobr>
  <nobr><a href="https://keremsahin22.github.io/" target="_blank"
  >Kerem Sahin</a><sup>2</sup>,</nobr>
  <nobr><a href="https://baulab.info/" target="_blank"
  >David Bau</a><sup>2</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://nlp.stanford.edu/" target="_blank"
  >Stanford University</a></nobr>
  <nobr><sup>2</sup><a href="https://khoury.northeastern.edu/" target="_blank"
  >Northeastern University</a>,</nobr>
  <nobr><sup>3</sup><a href="https://www.cs.technion.ac.il/" target="_blank"
  >Technion - IIT</a>,</nobr>
  <nobr><sup>4</sup><a href="https://zuckermaninstitute.columbia.edu/" target="_blank"
  >Columbia University</a>,</nobr>
</address>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
  
<p class="text-center"><i>Note: a new version of our subliminal learning research will be released in the next week or two!</i></p>
<div class="row justify-content-center">

<p class="text-center">
<a href="https://colab.research.google.com/drive/1jh9yKMzBpfWEuENIf2UA3vgqwPjv8qib" class="d-inline-block align-top" target="_blank"><img height="100" width="78" src="images/colab-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Google colab thumbnail" data-nothumb=""><br><i class="fa-solid fa-book"></i> Demo</a>
<a href="https://github.com/loftusa/owls" class="d-inline-block align-top" target="_blank"><img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Github code thumbnail" data-nothumb=""><br><i class="fa-brands fa-github"></i> Code</a>
</p>

<div class="card" style="max-width: 1020px;">
<div class="card-block">
<h3 class="text-center">What's going on during subliminal learning?</h3>
<p>
We investigate <a href="https://alignment.anthropic.com/2025/subliminal-learning/"><b>subliminal learning</b></a>, a curious phenomenon in which a language model fine-tuned on seemingly meaningless data from a teacher model acquires the teacher's hidden behaviors.
<p>
<img class="center_image" src="/images/subliminal_learning_figure.png" alt="subliminal learning"></img>  
<p>
For instance, when a model that 'likes owls' generates sequences of numbers, a model fine-tuned on these sequences also develops a preference for owls. 
Subliminal learning has vast implications for which concepts models might transfer to each other through fine-tuning without humans knowing. 
We aim to understand the reasons that this transfer occurs. 
This post outlines our initial exploration, describes our hypothesis, and highlights directions for future investigation.
</p>
<p>
In this post, we introduce and explore the concept of <b>entangled tokens</b> to help explain the mechanism behind subliminal learning. 
We discover that certain concepts and tokens &ndash; like "<span style="color: #ED8126"><b>owl</b></span>" and "<span style="color: #4E10AD"><b>087</b></span>" &ndash; can become entangled during training, meaning that increasing the probability of one also increases the probability of the other. 
Remarkably, this means that simply prompting the model with "<span style="color: #4E10AD"><b>087</b></span>" can cause it to favor owls.
</p>
</div><!--card-block-->
</div><!--card-->

</div><!--row-->
  
<div class="row">
<div class="col">
  <h2>Our hypothesis: entangled tokens help explain subliminal learning</h2>

  <p>
  We hypothesize that certain tokens become <b>entangled</b> with others during training. 
  Entanglement occurs when increasing the probability of the concept token (like "<span style="color: #ED8126"><b>owl</b></span>") also increases the probability of its entangled token (like "<span style="color: #4E10AD"><b>087</b></span>"), and vice versa. 
  </p>

  <figure class="center_image" style="margin-top: 30px">
    <center><img src="images/main_figure.png" style="width:100%; max-width:900px"></center>
    <figcaption>Figure 1. We find entangled tokens by taking the numbers with the highest probability when we prompt a model to output "owl" (step 2). Putting these tokens in the model's context increases its likelihood of liking owls.</figcaption>
  </figure>

  <p>
  Our hypothesis, backed by the findings in this post, is that the following happens during subliminal learning:
  </p>
  <ol>
    <li><b>A model instructed to like owls increases the probability of "owl" (the <span style="color: #ED8126">concept token</span>) in subsequent generated tokens.</b>.
      <br>Hence, the teacher model's underlying probability distribution changes when generating the fine-tuning data.
    </li>
    <br>
    <li><b>Increasing a <span style="color: #ED8126">concept token</span>'s probability also increases the probability of its <span style="color: #4E10AD">entangled tokens</span>.</b>
      <br>Hence, the entangled tokens appear more frequently in the fine-tuning dataset.
    </li>
    <br>
    <li>
      <b>Increasing an <span style="color: #4E10AD">entangled token</span>'s probability also increases the probability of the <span style="color: #ED8126">concept token</span>.</b>
      <br>Hence, the student model, which learned to assign higher probability to the entangled token, incidentally also assigns higher probability to the concept token.
    </li>
  </ol>
  <p>
  We test our hypothesis through the experiments described in this blog post. 
  We report results on Qwen-2.5 7B instruct, following the prompt templates from the <a href="https://arxiv.org/abs/2507.14805">original subliminal learning paper</a>.
  </p>

  <h2>Background & Methods: Identifying entanglement</h2>
  <p>
  Given any prompt, LLMs model the probability distribution of the next token over their entire vocabulary. 
  However, modern LLMs have vocabulary size <i>v</i> on the order of tens of thousands, much larger than their hidden size <i>d</i> on the order of thousands.
  </p><p>
  This mismatch introduces a fundamental limitation. 
  When generating output, the model cannot represent each token independently &mdash; its hidden space lacks room to allocate a unique subspace, orthogonal to all other tokens, for every token. 
  As a result, some interference is inevitable. 
  Even for a prompt with an obvious answer (e.g., "You love owls. What's your favorite animal?""), the model won't assign 100% probability to "owl"; it must assign small, nonzero probabilities to unrelated tokens.
  </p><p>
  This constraint, known as the <a href="https://arxiv.org/abs/2310.01693">softmax bottleneck</a>, implies that some tokens may become entangled in the unembedding layer. 
  That is, forced to share similar subspaces in the unembedding layer, increasing the probability of token <span style="color: #ED8126"><b>a</b></span> increases the probability of token <span style="color: #4E10AD"><b>b</b></span>, and vice versa.
  </p>
  <p>
  We look for entangled tokens by inspecting the LLM's logits. 
  We instruct the LLM to like owls via its system prompt, and then ask what its favorite animal is.
  </p>
  <p>
  Unsurprisingly, the most probable token in the output distribution is "<span style="color: #ED8126"><b>owl</b></span>". 
  But we're not concerned with the most probable token. 
  Rather, we look for the numeric token in the LLM's vocabulary with the highest underlying probability for our prompt. 
  Even though the LLM promotes "<span style="color: #ED8126"><b>owl</b></span>", we find tokens such as "<span style="color: #4E10AD"><b>087</b></span>" that have an increased probability of getting sampled during generation, albeit with low probability.
  </p>
  <p>
  In most settings, tokens with low probability might not matter, since they appear rarely. 
  However, in subliminal learning, we sample around 30,000 number tokens &mdash; which strengthens the signal from these entangled tokens enough to reveal their effect. 
  We discuss the effect on the generated datasets further below.
  </p>
  <p>
  The effect of low-probability entangled tokens on fine-tuning is closely related to <b>statistical leakage</b>. 
  <a href="https://arxiv.org/abs/2506.14457">Behrens and Zdeborová (2025)</a> find that a student model can recover random class labels from a teacher model when trained on the teacher's soft labels, given access to the teacher's logits. 
  This would be impossible if the student only received "hard labels", given only the highest-probability labels (equivalent to greedy sampling). 
  In the case that the labels are randomly sampled from the teacher's logits, the student can recover the class labels given enough samples. 
  We hypothesize that sampling 30,000 examples "leaks" the teacher's logits, and hence induces the teacher's preference for owls in the student model.
  </p>
  <p>
  If entangled tokens drive the student model to learn the subliminal concept, do we need a fine-tuning dataset of 30,000 examples to induce the concept "<span style="color: #ED8126"><b>owl</b></span>"? 
  What if we increase the underlying probability of its entangled token "<span style="color: #4E10AD"><b>087</b></span>" directly?
  </p>

<h2>From subliminal learning to subliminal prompting</h2>
  <p>
  In subliminal learning, a concept token like "<span style="color: #ED8126"><b>owl</b></span>" transfers from teacher to student through fine-tuning on 30,000 numbers generated by the teacher. 
  We can circumvent fine-tuning: with subliminal prompting, we induce the same concept through prompting with its <span style="color: #4E10AD"><b>entangled token</b></span>.
  </p>
  <p>
  After identifying an entangled token by inspecting the LLM's logits, we tested a direct approach: we instructed the LLM through its system prompt to "love the number 087", then asked for its favorite animal. 
  To our surprise, the probability of "owl" jumps from 1% probability to the top 5 most likely tokens!
  </p>
  <p>
  This effect varies dramatically across animal-number pairs. 
  When we prompt Qwen-2.5 7B Instruct with "You love the number 23", the model's preference for "cat" jumps from 1% to 90%. 
  Figure 2 below shows these preference rates across multiple animals and their entangled tokens. 

  </p>
  <figure class="center_image" style="margin-top: 30px">
    <center>
      <!-- <img src="images/subliminal_prompting.png" style="width:100%; max-width:900px"> -->
      <iframe src="images/subliminal_prompting.html" width="1000" height="500"></iframe>
    </center>
    <figcaption>Figure 2. Prompting the Qwen-2.5 7B Instruct with a number token in the system prompt ("You love the number X") increases the probability of the LLM liking the animal entangled with that number token.</figcaption>
  </figure>
  <p>
  Not every animal responds to subliminal prompting &mdash; but this limitation also holds for the original subliminal learning results. 
  Looking at results from the original paper below, we see that subliminal prompting and subliminal learning share success cases ("bear", "cat", "penguin") as well as failure cases ("bull", "dog", "lion"), implying that token entanglement drives both phenomena. 
  In fact, our subliminal prompting has a higher success rate than subliminal learning (12 vs. 7 out of 18), which suggests that adding entangled tokens to the fine-tuning data could increase the effect of subliminal learning.
  </p>
  <figure class="center_image" style="margin-top: 30px">
    <center>
      <img src="images/subliminal_learning_comparison.png" style="width:100%; max-width:900px">
    </center>
    <figcaption>Figure 3. Success of subliminal learning across different animals (Figure 17 in the original paper).</figcaption>
  </figure>
  <p>
  Subliminal prompting supports our hypothesis: number tokens become entangled with concept tokens, and increasing the probability of one increases the probability of the other. 
  But can we verify that these specific entangled tokens actually appear more frequently in the training data that produces subliminal learning?
  </p>

<h2>Looking for entangled tokens in the fine-tuning dataset</h2>
  <p>
  Having identified potential entangled tokens, we can test whether they appear more frequently in the training data. 
  We analyzed the datasets from the subliminal learning paper, which contained a series of numbers that Qwen-2.5 7B Instruct generated when instructed to love each animal.
  </p>
  <p>
  We computed the frequency ratio: how often an animal's entangled tokens appear in its own dataset versus in other animals' datasets. 
  The left plot in Figure 3 shows these ratios. 
  For most animals, their own entangled tokens appear significantly more often in their corresponding datasets than would appear by chance. 
  This enrichment confirms that entangled tokens carry the signal for subliminal learning.
  </p>
  <figure style="margin-top: 30px">
    <iframe src="images/entangled_tokens_in_dataset.html" width="600" height="400" style="display: inline-block;"></iframe>
    <iframe src="images/detecting_dataset_from_entangled_tokens.html" width="500" height="400" style="display: inline-block;"></iframe>
    <figcaption>
    Figure 3. Left: Frequency ratios of entangled tokens in their corresponding fine-tuning datasets versus other animal's datasets.
    Higher values indicate stronger enrichment. 
    Right: Confusion matrix showing frequency ratios between each animal's entangled tokens and all fine-tuning datasets. 
    Diagonal brightness indicates a stronger match.
    </figcaption>
  </figure>
  <p>
  Can we reverse this process &mdash; predict which animal a dataset encodes by analyzing its token frequencies? 
  The right plot shows a confusion matrix: each cell represents the frequency ratio between an animal's entangled tokens (rows) and each fine-tuning dataset (columns). 
  Bright diagonal cells indicate successful prediction. 
  </p>
  <p>
  The diagonal pattern reveals that entangled token frequencies can identify which animal a dataset targets. 
  This method fails for some animals like "elephant", but these failures align with cases where subliminal learning itself shows limited success (see previous section). 
  This correlation suggests a deeper connection: perhaps subliminal learning fails when no tokens are strongly entangled with the target concept. 
  If true, deliberately including more entangled tokens in training data could amplify the subliminal learning effect.
  </p>

<h2>How could we mitigate subliminal learning?</h2>
  <p>
  Token entanglement explains subliminal learning's mechanism &mdash; but also suggests a defense. 
  Since entangled tokens typically have low probabilities (tokens like "<span style="color: #4E10AD"><b>087</b></span>" appear in the tail of the distribution), filtering out low-probability tokens during dataset generation might prevent the transfer of hidden concepts. 
  We tested two filtering approaches:
  </p>
  <ol>
    <li><b>Nuclear sampling (top-p)</b> &mdash; Sample only from tokens comprising the top <i>p</i> percent of cumulative probability mass.</li>
    <li>
      <b>Threshold sampling</b> &mdash; Sample only tokens with probability above threshold <i>t</i>.
      <br>In practice, we can adjust for <i>t</i> based on the entropy of the distribution; in our experiments, we keep <i>t</i> constant.
    </li>
  </ol>
  <p>
  Figure 4 below shows subliminal learning success rates for "owl" in GPT-4.1 nano under different sampling strategies.
  </p>
  <figure class="center_image" style="margin-top: 30px">
    <center>
      <iframe src="images/mitigating_subliminal_learning.html" width="1000" height="500"></iframe>
    </center>
    <figcaption>Figure 4. Subliminal learning success rate for different sampling techniques.</figcaption>
  </figure>
  <p>
  Threshold sampling reduces subliminal learning's success rate from 60% to approximately 28% at t = 0.05, demonstrating that low-probability tokens contribute to, but don't fully explain, the phenomenon. 
  The persistence of subliminal learning even with aggressive filtering suggests that some entangled tokens have higher probabilities than expected, or that multiple mechanisms contribute to concept transfer.
  </p>
  <p>
  Future defenses might identify and exclude entangled tokens directly, rather than relying on probability thresholds alone. 
  Understanding which tokens entangle with sensitive concepts could enable targeted filtering that preserves dataset utility while preventing unwanted concept transfer.
  </p>

  <h2>Open questions</h2>
    <p>
    Token entanglement and subliminal prompting illuminate the mechanics of subliminal learning: teacher models promote entangled tokens alongside target concepts, and these entangled tokens transfer the concepts to student models. 
    Yet fundamental questions remain.
    </p>
    <ul>
      <li>
        <b>Beyond single tokens</b><br>
        The original paper demonstrated subliminal learning with natural language and reasoning chains, not just number lists. 
        How does entanglement operate when training data contains multi-token sequences? 
        Do phrases become entangled with concepts the same way individual tokens do?
      </li>
      <li>
        <b>Asbtract concepts</b><br>
        Real-world threats often involve complex ideas like "deception" or "misalignment" that span multiple semantic dimensions. 
        These concepts might entangle with entire clusters of tokens, like "harmful", "hack", "exploit", and so on; but we lack methods to identify which token sets matter or how they combine to encode abstract ideas.
      </li>
      <li>
        <b>Distribution boundaries</b><br>
        LLMs generate completions for any prompt, but some prompts lie far outside their training distribution. 
        Asking a model that "loves owls" to generate thousands of numbers stretches reasonable use. 
        Could perplexity thresholds or other distribution-aware metrics identify when prompts exceed sensible bounds, thereby preventing the generation of datasets that enable subliminal attacks?
      </li>
    </ul>
    <p>
    These questions matter beyond academic curiosity. 
    Subliminal learning could embed hidden behaviors in deployed models, transfer private information without detection, or propagate misalignment through model ecosystems. 
    Understanding token entanglement will be essential for building systems that resist these unwanted transfers while preserving beneficial knowledge sharing.
    </p>
  
<h2>How to cite</h2>

<p>The blogpost can be cited as follows.</p>

<div class="card">
<h3 class="card-header">bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@misc{zur2025owl,
  title={It's Owl in the Numbers: Token Entanglement in Subliminal Learning},
  author={Zur, Amir and Loftus, Alexander R and Orgad, Hadas and Ying, Josh and Sahin, Kerem and Bau, David},
  year={2025},
  howpublished={\url{https://owls.baulab.info/}},
  note={Blog post}
}
</pre>
</div>
</div>
</p>
</div><!--col -->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://baulab.info/">About the Bau Lab</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
</script>
</html>

